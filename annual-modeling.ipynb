{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 4: Annual Modeling \n",
    "In this notebook, we will try modeling gun violence trends annually. Since we have more annual data than monthly data, our goal is to see whether the annual features will be more helpful here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import defaultdict\n",
    "from collections import defaultdict\n",
    "\n",
    "# Numpy and pandas for manipulating the data\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Matplotlib and seaborn for visualization\n",
    "import matplotlib as mpl\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# GridSearchCV for training \n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Performance metrics from sklearn\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# Prophet for time forecasting\n",
    "from fbprophet import Prophet\n",
    "\n",
    "# Classification models\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "# To hide stdout because Prophet can be loud\n",
    "import logging\n",
    "logging.getLogger('fbprophet').setLevel(logging.WARNING)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "annual_file = './data/cleaned/annual.csv'\n",
    "by_date_total_file = './data/cleaned/by_date_total.csv'\n",
    "provisions_file = './data/raw/provisions.csv'\n",
    "useful_provisions_file = './data/cleaned/useful_provisions.csv'\n",
    "\n",
    "annual_df = pd.read_csv(annual_file, parse_dates=True, index_col=0)\n",
    "by_date_total_df = pd.read_csv(by_date_total_file, parse_dates=True, index_col=0)\n",
    "provisions_df = pd.read_csv(provisions_file, parse_dates=True)\n",
    "useful_provisions_df = pd.read_csv(useful_provisions_file, parse_dates=True, index_col=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fixing up features\n",
    "As before, there are still a couple of features to be tweaked. We need to remove redundant columns and add the label and the provisions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Drop District of Columbia\n",
    "annual_df = annual_df[annual_df['state'] != 'District of Columbia']\n",
    "\n",
    "# Drop redundant columns\n",
    "annual_df = annual_df.drop(['gun_deaths_norm', 'other_crime_norm'], axis=1)\n",
    "\n",
    "# Get states that we will be making models for\n",
    "states = annual_df['state'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create columns for next year's gun deaths (what we want to predict for each row)\n",
    "next_annual_df = pd.DataFrame()\n",
    "next_annual_df['state'] = annual_df['state']\n",
    "next_annual_df['this_year'] = annual_df['year'] - 1\n",
    "next_annual_df['next_year'] = annual_df['year']\n",
    "next_annual_df['next_gun_deaths'] = annual_df['gun_deaths']\n",
    "\n",
    "# Merge so we have next year's gun deaths in each row\n",
    "annual_df = pd.merge(annual_df, next_annual_df,\n",
    "         left_on=['year', 'state'], right_on=['this_year', 'state'])\n",
    "\n",
    "# Drop year, as it is now named this_year\n",
    "annual_df = annual_df.drop('year', axis=1)\n",
    "\n",
    "# Add features from last year in order to view short time trend\n",
    "features_to_add = annual_df.drop(['this_year', 'next_year',\n",
    "                                  'state', 'next_gun_deaths'], axis=1).columns\n",
    "\n",
    "last_annual_df = pd.DataFrame()\n",
    "last_annual_df['state'] = annual_df['state']\n",
    "last_annual_df['this_year'] = annual_df['this_year'] + 1\n",
    "last_annual_df['last_year'] = annual_df['this_year']\n",
    "for feature in features_to_add:\n",
    "    last_annual_df['last_' + feature] = annual_df[feature]\n",
    "\n",
    "annual_df = pd.merge(annual_df, last_annual_df,\n",
    "                    left_on=['this_year', 'state'], right_on=['this_year', 'state'])\n",
    "\n",
    "# Replace last year value with change from last year\n",
    "for feature in features_to_add:\n",
    "    current = annual_df[feature]\n",
    "    last = annual_df['last_' + feature]\n",
    "    annual_df[feature + '_change'] = (current - last) / np.clip(last, 1, None)\n",
    "    annual_df = annual_df.drop('last_' + feature, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "157"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a label (gun deaths increases by more than 10%)\n",
    "rate_change = (annual_df['next_gun_deaths'] - annual_df['gun_deaths'] ) / annual_df['gun_deaths']\n",
    "annual_df['label'] = rate_change > 0.2\n",
    "annual_df['label'].sum() # See how many positives we have"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Add only the useful provisions to our annual_df (k from this year and k from n years prior)\n",
    "def add_provisions(annual_df, provisions_df, useful_provisions_df, k=30, n=5):\n",
    "    # Get the state and year columns for a join later and lawtotal to account for excluded provisions\n",
    "    columns = list(useful_provisions_df.head(k)['provision'].values)\n",
    "    columns.extend(['year', 'state', 'lawtotal'])     \n",
    "    \n",
    "    # Get the years \n",
    "    years = annual_df.groupby('this_year').count().index.values\n",
    "\n",
    "    # Keep track of provisions for this year and n years prior\n",
    "    current_provisions = []\n",
    "    old_provisions = []\n",
    "\n",
    "    # Add the provisions from each year to a list\n",
    "    for year in years:\n",
    "        current_provisions.append(provisions_df[provisions_df['year'] == year][columns])\n",
    "        old_provisions.append(provisions_df[provisions_df['year'] == year - n][columns])\n",
    "\n",
    "    # Put the provisions into a DataFrame\n",
    "    current_provisions = pd.concat(current_provisions)\n",
    "    old_provisions = pd.concat(old_provisions)\n",
    "    old_provisions['year'] += n # Match the year which we want to join onto\n",
    "\n",
    "    # Merge the provisions\n",
    "    all_provisions = pd.merge(current_provisions, old_provisions, \n",
    "                              on=['state', 'year'], suffixes=('', '_old'))\n",
    "\n",
    "    # Add provisions to annual_df and return the new annual_df\n",
    "    annual_df = pd.merge(annual_df, all_provisions, \n",
    "                          left_on=['this_year', 'state'], \n",
    "                          right_on=['year', 'state'])\n",
    "    return annual_df.drop('year', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Add provisions to our annual_df\n",
    "annual_df = add_provisions(annual_df, provisions_df, useful_provisions_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Make a function to train our models and return the results\n",
    "def train_models(annual_df, models, test_year, extra_columns):\n",
    "    \"\"\" Function to train models, returning test and train predictions and trained models.\n",
    "    feature_df   (DataFrame): Pandas DataFrame with all of the features, including the label\n",
    "    \n",
    "    models            (dict): dict with model names as keys and model, params pairs as values \n",
    "    \n",
    "    test_year          (int): Year to test on\n",
    "    \n",
    "    extra_columns     (list): List of columns to drop before training (columns that would either \n",
    "    not help with the predictions, or would be cheating by using the label itself). \n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    # Initialize the dictionaries we will be returning later\n",
    "    training_history = defaultdict(list)\n",
    "    testing_history = defaultdict(list)\n",
    "    testing_history_probs = defaultdict(list)\n",
    "    trained_models = defaultdict(dict)\n",
    "    \n",
    "    # Train XGBoost for next_year 2000 to 2013; make predictions for 2014-2017\n",
    "    model, parameters = models['XGBoost']\n",
    "    \n",
    "    pretrain_preds = []\n",
    "    for year in [2014, 2015, 2016, 2017]:\n",
    "        train_filter = annual_df['next_year'] < year\n",
    "        test_filter = annual_df['next_year'] == year\n",
    "    \n",
    "        X_train = annual_df.dropna(axis=1).loc[train_filter].drop(extra_columns, axis=1).values\n",
    "        y_train = annual_df.loc[train_filter, 'label']\n",
    "\n",
    "        X_test = annual_df.dropna(axis=1).loc[test_filter].drop(extra_columns, axis=1).values\n",
    "        y_test = annual_df.loc[train_filter, 'label']\n",
    "            \n",
    "        clf = GridSearchCV(model, parameters)\n",
    "        clf.fit(X_train, y_train)\n",
    "\n",
    "        # Make predictions on training set\n",
    "        test_preds = clf.predict(X_test)\n",
    "        pretrain_preds.extend(test_preds)\n",
    "\n",
    "    \n",
    "    feature_df = annual_df.dropna()\n",
    "    print(feature_df.shape)\n",
    "    print(len(pretrain_preds))\n",
    "    feature_df['pretrain'] = pretrain_preds\n",
    "    \n",
    "    # For each state, train a new set of models\n",
    "    # Training data is all data before next_year\n",
    "    # Testing data all data during next_year\n",
    "    train_filter = feature_df['next_year'] < test_year\n",
    "    test_filter = feature_df['next_year'] >= test_year\n",
    "\n",
    "    # Partition the feature_df for the training and testing sets\n",
    "    X_train = feature_df.loc[train_filter].drop(extra_columns, axis=1).values\n",
    "    y_train = feature_df.loc[train_filter, 'label']\n",
    "\n",
    "    # Note that the test set has only ONE row for each state. \n",
    "    X_test = feature_df.loc[test_filter].drop(extra_columns, axis=1).values\n",
    "    y_test = feature_df.loc[test_filter, 'label']\n",
    "\n",
    "    # Keep track of predictions so we can train the meta model as well\n",
    "    meta_train = []\n",
    "    meta_test = []\n",
    "    for name, (model, parameters) in models.items():\n",
    "        clf = GridSearchCV(model, parameters)\n",
    "        clf.fit(X_train, y_train)\n",
    "\n",
    "        # Make predictions on training set\n",
    "        train_preds = clf.predict(X_train)\n",
    "        test_preds = clf.predict(X_test)\n",
    "        train_probs = clf.best_estimator_.predict_proba(X_train)[:, 0]\n",
    "        test_probs = clf.best_estimator_.predict_proba(X_test)[:, 0]\n",
    "\n",
    "        # Make meta features to train the meta model on\n",
    "        meta_train.append(train_probs)\n",
    "        meta_test.append(test_probs)\n",
    "\n",
    "        # Keep track of the predictions\n",
    "        training_history[name].append(train_preds)\n",
    "        testing_history[name].extend(test_preds)\n",
    "        testing_history_probs[name].extend(test_probs)\n",
    "\n",
    "        # Remember the last model\n",
    "        trained_models[name] = clf\n",
    "\n",
    "    # Take transpose of meta features so that observations are rows\n",
    "    meta_train = np.array(meta_train).T\n",
    "    meta_test = np.array(meta_test).T\n",
    "\n",
    "    # Create and train the meta model\n",
    "    clf = GridSearchCV(XGBClassifier(), xgb_params)\n",
    "    clf.fit(meta_train, y_train)\n",
    "\n",
    "    # Make training and testing predictions\n",
    "    train_preds = clf.predict(meta_train)\n",
    "    test_preds = clf.predict(meta_test)\n",
    "\n",
    "    # Keep track of the predictions\n",
    "    training_history['meta'].append(train_preds)\n",
    "    testing_history['meta'].extend(test_preds)\n",
    "    testing_history_probs['meta'].extend(test_probs)\n",
    "    return training_history, testing_history, testing_history_probs, trained_models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Training models\n",
    "At this point we should make an important decision. We have to decide the years which we will be training on, as some of our data is not available for earlier years. In this first test, I decided to drop the features that we have insufficient data for, and just training on the features that have complete data for all years."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Prepare feature_df and extra columns\n",
    "extra_columns = ['label', 'next_gun_deaths']\n",
    "\n",
    "# Make dummies for states\n",
    "annual_df = pd.get_dummies(annual_df, columns=['state'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Make our models that we want to train\n",
    "# Parameters for XGBClassifier\n",
    "xgb_params = {\n",
    "  'max_depth': [3, 5, 7, 9], \n",
    "  'n_estimators': [30, 50, 100, 300]\n",
    "}\n",
    "\n",
    "# Parameters for LogisitcRegression\n",
    "logi_regr_params = {\n",
    "    'penalty': ['l1', 'l2'],\n",
    "    'C': [1e-2, 1e-1, 1, 10, 1e3, 1e5]\n",
    "}\n",
    "\n",
    "# Parameters for RandomForest\n",
    "random_forest_params = {\n",
    "  'max_depth': [3, 5, 7, 9],\n",
    "  'n_estimators': [30, 50, 100, 300]\n",
    "}\n",
    "\n",
    "# Parameters for AdaBoost\n",
    "adaboost_params = {\n",
    "  'n_estimators': [30, 50, 100, 300]\n",
    "}\n",
    "\n",
    "knn_params = {\n",
    "    'n_neighbors': [3, 5, 7],\n",
    "    'algorithm': ['ball_tree', 'kd_tree']\n",
    "}\n",
    "\n",
    "# Parameters for GaussianNB\n",
    "percent_positive = annual_df['label'].mean() # Percentage of positive labels\n",
    "percent_negative = 1 - percent_positive # Percentage of negative features \n",
    "bayes_params = {'priors': [None, [percent_negative, percent_positive]]}\n",
    "\n",
    "\n",
    "# Create a dictionary of models with names as keys\n",
    "# model{ 'model name': (model_object, parameters) } \n",
    "models = {\n",
    "    'XGBoost': (XGBClassifier(), xgb_params), \n",
    "    'Logistic Reg': (LogisticRegression(), logi_regr_params),\n",
    "    'Random Forest': (RandomForestClassifier(), random_forest_params),\n",
    "    'AdaBoost': (AdaBoostClassifier(), adaboost_params),\n",
    "    'KNN' : (KNeighborsClassifier(), knn_params),\n",
    "    'Gaussian NB': (GaussianNB(), bayes_params)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-a505410f43cc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mtest_year\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m2016\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m (training_history, testing_history, \n\u001b[0;32m----> 3\u001b[0;31m testing_history_probs, trained_models) = train_models(annual_df, models, test_year, extra_columns)\n\u001b[0m",
      "\u001b[0;32m<ipython-input-8-57e8720b58e5>\u001b[0m in \u001b[0;36mtrain_models\u001b[0;34m(annual_df, models, test_year, extra_columns)\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m         \u001b[0mclf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGridSearchCV\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparameters\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m         \u001b[0mclf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m         \u001b[0;31m# Make predictions on training set\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[1;32m    637\u001b[0m                                   error_score=self.error_score)\n\u001b[1;32m    638\u001b[0m           for parameters, (train, test) in product(candidate_params,\n\u001b[0;32m--> 639\u001b[0;31m                                                    cv.split(X, y, groups)))\n\u001b[0m\u001b[1;32m    640\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    641\u001b[0m         \u001b[0;31m# if one choose to see train score, \"out\" will contain train score info\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m    777\u001b[0m             \u001b[0;31m# was dispatched. In particular this covers the edge\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    778\u001b[0m             \u001b[0;31m# case of Parallel used with an exhausted iterator.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 779\u001b[0;31m             \u001b[0;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatch_one_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    780\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    781\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36mdispatch_one_batch\u001b[0;34m(self, iterator)\u001b[0m\n\u001b[1;32m    623\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    624\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 625\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dispatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtasks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    626\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    627\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36m_dispatch\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    586\u001b[0m         \u001b[0mdispatch_timestamp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    587\u001b[0m         \u001b[0mcb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBatchCompletionCallBack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdispatch_timestamp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 588\u001b[0;31m         \u001b[0mjob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    589\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jobs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    590\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/sklearn/externals/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36mapply_async\u001b[0;34m(self, func, callback)\u001b[0m\n\u001b[1;32m    109\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mapply_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m         \u001b[0;34m\"\"\"Schedule a func to be run\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 111\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImmediateResult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    112\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m             \u001b[0mcallback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/sklearn/externals/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    330\u001b[0m         \u001b[0;31m# Don't delay the application, to avoid keeping the input\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    331\u001b[0m         \u001b[0;31m# arguments in memory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 332\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    333\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    334\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 131\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    132\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 131\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    132\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/sklearn/model_selection/_validation.py\u001b[0m in \u001b[0;36m_fit_and_score\u001b[0;34m(estimator, X, y, scorer, train, test, verbose, parameters, fit_params, return_train_score, return_parameters, return_n_test_samples, return_times, error_score)\u001b[0m\n\u001b[1;32m    456\u001b[0m             \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    457\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 458\u001b[0;31m             \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    459\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    460\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/python-libraries/xgboost/python-package/xgboost/sklearn.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight, eval_set, eval_metric, early_stopping_rounds, verbose, xgb_model)\u001b[0m\n\u001b[1;32m    504\u001b[0m                               \u001b[0mearly_stopping_rounds\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mearly_stopping_rounds\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    505\u001b[0m                               \u001b[0mevals_result\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mevals_result\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeval\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeval\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 506\u001b[0;31m                               verbose_eval=verbose, xgb_model=None)\n\u001b[0m\u001b[1;32m    507\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    508\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobjective\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mxgb_options\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"objective\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/python-libraries/xgboost/python-package/xgboost/training.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(params, dtrain, num_boost_round, evals, obj, feval, maximize, early_stopping_rounds, evals_result, verbose_eval, xgb_model, callbacks, learning_rates)\u001b[0m\n\u001b[1;32m    202\u001b[0m                            \u001b[0mevals\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mevals\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m                            \u001b[0mobj\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeval\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeval\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 204\u001b[0;31m                            xgb_model=xgb_model, callbacks=callbacks)\n\u001b[0m\u001b[1;32m    205\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    206\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/python-libraries/xgboost/python-package/xgboost/training.py\u001b[0m in \u001b[0;36m_train_internal\u001b[0;34m(params, dtrain, num_boost_round, evals, obj, feval, xgb_model, callbacks)\u001b[0m\n\u001b[1;32m     72\u001b[0m         \u001b[0;31m# Skip the first update if it is a recovery step.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mversion\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 74\u001b[0;31m             \u001b[0mbst\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     75\u001b[0m             \u001b[0mbst\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_rabit_checkpoint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m             \u001b[0mversion\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/python-libraries/xgboost/python-package/xgboost/core.py\u001b[0m in \u001b[0;36mupdate\u001b[0;34m(self, dtrain, iteration, fobj)\u001b[0m\n\u001b[1;32m    896\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mfobj\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m             _check_call(_LIB.XGBoosterUpdateOneIter(self.handle, ctypes.c_int(iteration),\n\u001b[0;32m--> 898\u001b[0;31m                                                     dtrain.handle))\n\u001b[0m\u001b[1;32m    899\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    900\u001b[0m             \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtrain\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "test_year = 2016\n",
    "(training_history, testing_history, \n",
    "testing_history_probs, trained_models) = train_models(annual_df, models, test_year, extra_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "all_preds = []\n",
    "for v in testing_history.values():\n",
    "    all_preds.append(v)\n",
    "    \n",
    "all_preds = np.array(all_preds).T\n",
    "vote_by_preds = [int(x > 0.5) for x in all_preds.mean(axis=1)]\n",
    "\n",
    "testing_history['vote_by_preds'] = vote_by_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "date_filter = (annual_df['next_year'] >= 2016)\n",
    "truth = annual_df.loc[date_filter, 'label']\n",
    "\n",
    "testing_results_df = pd.DataFrame(annual_df.loc[date_filter, ['label', 'next_date']])\n",
    "for name, preds in testing_history.items():\n",
    "    # Add predictions to feature_df\n",
    "    testing_results_df[name] = preds == truth\n",
    "    \n",
    "    # Get accuracy score and confusion matrix\n",
    "    print(\"{}: {} \".format(name, accuracy_score(truth, preds)))\n",
    "    cm = confusion_matrix(truth, preds)\n",
    "    tn, fp, fn, tp = cm[0][0], cm[0][1], cm[1][0], cm[1][1]\n",
    "    recall = tp / (tp + fn)\n",
    "    precision = tp / (tp + fp)\n",
    "#     print(\"True Negative: {}\".format(tn))\n",
    "#     print(\"False Positive: {}\".format(fp))\n",
    "#     print(\"False Negative: {}\".format(fn))\n",
    "#     print(\"True Positive: {}\".format(tp))\n",
    "    print(\"Recall: {}**\".format(recall))\n",
    "    print(\"Precision: {}\".format(precision))\n",
    "    print(cm)\n",
    "    print('-'*10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 4: Annual Modeling \n",
    "In this notebook, we will try modeling gun violence trends annually. Since we have more annual data than monthly data, our goal is to see whether the annual features will be more helpful here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import defaultdict\n",
    "from collections import defaultdict\n",
    "\n",
    "# Numpy and pandas for manipulating the data\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Matplotlib and seaborn for visualization\n",
    "import matplotlib as mpl\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# GridSearchCV for training \n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Performance metrics from sklearn\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# Prophet for time forecasting\n",
    "from fbprophet import Prophet\n",
    "\n",
    "# Classification models\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "# To hide stdout because Prophet can be loud\n",
    "import logging\n",
    "logging.getLogger('fbprophet').setLevel(logging.WARNING)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "annual_file = './data/cleaned/annual.csv'\n",
    "by_date_total_file = './data/cleaned/by_date_total.csv'\n",
    "provisions_file = './data/raw/provisions.csv'\n",
    "useful_provisions_file = './data/cleaned/useful_provisions.csv'\n",
    "\n",
    "annual_df = pd.read_csv(annual_file, parse_dates=True, index_col=0)\n",
    "by_date_total_df = pd.read_csv(by_date_total_file, parse_dates=True, index_col=0)\n",
    "provisions_df = pd.read_csv(provisions_file, parse_dates=True)\n",
    "useful_provisions_df = pd.read_csv(useful_provisions_file, parse_dates=True, index_col=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fixing up features\n",
    "As before, there are still a couple of features to be tweaked. We need to remove redundant columns and add the label and the provisions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop District of Columbia\n",
    "annual_df = annual_df[annual_df['state'] != 'District of Columbia']\n",
    "\n",
    "# Drop redundant columns\n",
    "annual_df = annual_df.drop(['gun_deaths_norm', 'other_crime_norm'], axis=1)\n",
    "\n",
    "# Get states that we will be making models for\n",
    "states = annual_df['state'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create columns for next year's gun deaths (what we want to predict for each row)\n",
    "next_annual_df = pd.DataFrame()\n",
    "next_annual_df['state'] = annual_df['state']\n",
    "next_annual_df['this_year'] = annual_df['year'] - 1\n",
    "next_annual_df['next_year'] = annual_df['year']\n",
    "next_annual_df['next_gun_deaths'] = annual_df['gun_deaths']\n",
    "\n",
    "annual_df = pd.merge(annual_df, next_annual_df,\n",
    "         left_on=['year', 'state'], right_on=['this_year', 'state'])\n",
    "\n",
    "annual_df = annual_df.drop('this_year', axis=1)\n",
    "\n",
    "# Add features from last year in order to view short time trend\n",
    "features_to_add = annual_df.drop(['year', 'this_year', 'next_year',\n",
    "                                  'state', 'next_gun_deaths'], axis=1).columns\n",
    "\n",
    "last_annual_df = pd.DataFrame()\n",
    "last_annual_df['state'] = annual_df['state']\n",
    "last_annual_df['this_year'] = annual_df['year'] + 1\n",
    "last_annual_df['last_year'] = annual_df['year']\n",
    "for feature in features_to_add:\n",
    "    last_annual_df['last_' + feature] = annual_df[feature]\n",
    "\n",
    "annual_df = pd.merge(annual_df, last_annual_df,\n",
    "                    left_on=['year', 'state'], right_on=['this_year', 'state'])\n",
    "annual_df = annual_df.drop('year', axis=1)\n",
    "\n",
    "# Replace last year value with change from last year\n",
    "for feature in features_to_add:\n",
    "    current = annual_df[feature]\n",
    "    last = annual_df['last_' + feature]\n",
    "    annual_df[feature + '_change'] = (current - last) / last\n",
    "    annual_df = annual_df.drop('last_' + feature, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "state\n",
      "gun_deaths\n",
      "population\n",
      "violent_crime\n",
      "property_crime\n",
      "murder_crime\n",
      "rape_crime\n",
      "robbery_crime\n",
      "assault_crime\n",
      "burglary_crime\n",
      "larceny_theft_crime\n",
      "vehicle_theft_crime\n",
      "other_crime\n",
      "income\n",
      "beer\n",
      "wine\n",
      "spirits\n",
      "alcohol_consumed\n",
      "us_decile\n",
      "lawtotal\n",
      "democrat\n",
      "republican\n",
      "other_weapon\n",
      "destructive_device\n",
      "machinegun\n",
      "silencer\n",
      "short_barreled_rifle\n",
      "short_barreled_shotgun\n",
      "total_weapons\n",
      "marijuana\n",
      "cocaine\n",
      "tobacco\n",
      "alcohol_abuse\n",
      "mental\n",
      "depression\n",
      "this_year_x\n",
      "next_year\n",
      "next_gun_deaths\n",
      "this_year_y\n",
      "last_year\n",
      "gun_deaths_change\n",
      "population_change\n",
      "violent_crime_change\n",
      "property_crime_change\n",
      "murder_crime_change\n",
      "rape_crime_change\n",
      "robbery_crime_change\n",
      "assault_crime_change\n",
      "burglary_crime_change\n",
      "larceny_theft_crime_change\n",
      "vehicle_theft_crime_change\n",
      "other_crime_change\n",
      "income_change\n",
      "beer_change\n",
      "wine_change\n",
      "spirits_change\n",
      "alcohol_consumed_change\n",
      "us_decile_change\n",
      "lawtotal_change\n",
      "democrat_change\n",
      "republican_change\n",
      "other_weapon_change\n",
      "destructive_device_change\n",
      "machinegun_change\n",
      "silencer_change\n",
      "short_barreled_rifle_change\n",
      "short_barreled_shotgun_change\n",
      "total_weapons_change\n",
      "marijuana_change\n",
      "cocaine_change\n",
      "tobacco_change\n",
      "alcohol_abuse_change\n",
      "mental_change\n",
      "depression_change\n"
     ]
    }
   ],
   "source": [
    "for c in annual_df.columns:\n",
    "    print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "161"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a label (gun deaths increases by more than 10%)\n",
    "rate_change = (annual_df['next_gun_deaths'] - annual_df['gun_deaths'] ) / annual_df['gun_deaths']\n",
    "annual_df['label'] = rate_change > 0.2\n",
    "annual_df['label'].sum() # See how many positives we have"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Add only the useful provisions to our annual_df (k from this year and k from n years prior)\n",
    "def add_provisions(annual_df, provisions_df, useful_provisions_df, k=30, n=5):\n",
    "    # Get the state and year columns for a join later and lawtotal to account for excluded provisions\n",
    "    columns = list(useful_provisions_df.head(k)['provision'].values)\n",
    "    columns.extend(['year', 'state', 'lawtotal'])     \n",
    "    \n",
    "    # Get the years \n",
    "    years = annual_df.groupby('this_year').count().index.values\n",
    "\n",
    "    # Keep track of provisions for this year and n years prior\n",
    "    current_provisions = []\n",
    "    old_provisions = []\n",
    "\n",
    "    # Add the provisions from each year to a list\n",
    "    for year in years:\n",
    "        current_provisions.append(provisions_df[provisions_df['year'] == year][columns])\n",
    "        old_provisions.append(provisions_df[provisions_df['year'] == year - n][columns])\n",
    "\n",
    "    # Put the provisions into a DataFrame\n",
    "    current_provisions = pd.concat(current_provisions)\n",
    "    old_provisions = pd.concat(old_provisions)\n",
    "    old_provisions['year'] += n # Match the year which we want to join onto\n",
    "\n",
    "    # Merge the provisions\n",
    "    all_provisions = pd.merge(current_provisions, old_provisions, \n",
    "                              on=['state', 'year'], suffixes=('', '_old'))\n",
    "\n",
    "    # Add provisions to annual_df and return the new annual_df\n",
    "    annual_df = pd.merge(annual_df, all_provisions, \n",
    "                          left_on=['this_year', 'state'], \n",
    "                          right_on=['year', 'state'])\n",
    "    return annual_df.drop('year', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add provisions to our annual_df\n",
    "annual_df = add_provisions(annual_df, provisions_df, useful_provisions_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Make a function to train our models and return the results\n",
    "def train_models(feature_df, models, test_year, extra_columns):\n",
    "    \"\"\" Function to train models, returning test and train predictions and trained models.\n",
    "    feature_df   (DataFrame): Pandas DataFrame with all of the features, including the label\n",
    "    \n",
    "    models            (dict): dict with model names as keys and model, params pairs as values \n",
    "    \n",
    "    test_year          (int): Year to test on\n",
    "    \n",
    "    extra_columns     (list): List of columns to drop before training (columns that would either \n",
    "    not help with the predictions, or would be cheating by using the label itself). \n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    # Initialize the dictionaries we will be returning later\n",
    "    training_history = defaultdict(list)\n",
    "    testing_history = defaultdict(list)\n",
    "    testing_history_probs = defaultdict(list)\n",
    "    trained_models = defaultdict(dict)\n",
    "    \n",
    "    # For each state, train a new set of models\n",
    "    # Training data is all data before next_year\n",
    "    # Testing data all data during next_year\n",
    "    train_date_filter = feature_df['next_year'] < test_year\n",
    "    test_date_filter = feature_df['next_year'] >= test_year\n",
    "    train_filter = train_date_filter\n",
    "    test_filter = test_date_filter\n",
    "\n",
    "    # Partition the feature_df for the training and testing sets\n",
    "    X_train = feature_df.loc[train_filter].drop(extra_columns, axis=1).values\n",
    "    y_train = feature_df.loc[train_filter, 'label']\n",
    "\n",
    "    # Note that the test set has only ONE row for each state. \n",
    "    X_test = feature_df.loc[test_filter].drop(extra_columns, axis=1).values\n",
    "    y_test = feature_df.loc[test_filter, 'label']\n",
    "\n",
    "    # Keep track of predictions so we can train the meta model as well\n",
    "    meta_train = []\n",
    "    meta_test = []\n",
    "    for name, (model, parameters) in models.items():\n",
    "        clf = GridSearchCV(model, parameters)\n",
    "        clf.fit(X_train, y_train)\n",
    "\n",
    "        # Make predictions on training set\n",
    "        train_preds = clf.predict(X_train)\n",
    "        test_preds = clf.predict(X_test)\n",
    "        train_probs = clf.best_estimator_.predict_proba(X_train)[:, 0]\n",
    "        test_probs = clf.best_estimator_.predict_proba(X_test)[:, 0]\n",
    "\n",
    "        # Make meta features to train the meta model on\n",
    "        meta_train.append(train_probs)\n",
    "        meta_test.append(test_probs)\n",
    "\n",
    "        # Keep track of the predictions\n",
    "        training_history[name].append(train_preds)\n",
    "        testing_history[name].extend(test_preds)\n",
    "        testing_history_probs[name].extend(test_probs)\n",
    "\n",
    "        # Remember the last model\n",
    "        trained_models[name] = clf\n",
    "\n",
    "    # Take transpose of meta features so that observations are rows\n",
    "    meta_train = np.array(meta_train).T\n",
    "    meta_test = np.array(meta_test).T\n",
    "\n",
    "    # Create and train the meta model\n",
    "    clf = GridSearchCV(XGBClassifier(), xgb_params)\n",
    "    clf.fit(meta_train, y_train)\n",
    "\n",
    "    # Make training and testing predictions\n",
    "    train_preds = clf.predict(meta_train)\n",
    "    test_preds = clf.predict(meta_test)\n",
    "\n",
    "    # Keep track of the predictions\n",
    "    training_history['meta'].append(train_preds)\n",
    "    testing_history['meta'].extend(test_preds)\n",
    "    testing_history_probs['meta'].extend(test_probs)\n",
    "    return training_history, testing_history, testing_history_probs, trained_models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Training models\n",
    "At this point we should make an important decision. We have to decide the years which we will be training on, as some of our data is not available for earlier years. In this first test, I decided to drop the features that we have insufficient data for, and just training on the features that have complete data for all years."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_use = annual_df.isnull().sum().index[annual_df.isnull().sum() < 100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare feature_df and extra columns\n",
    "feature_df = annual_df[to_use].dropna()\n",
    "extra_columns = ['label', 'next_gun_deaths', 'state']\n",
    "\n",
    "# Remember states because we need it when we train a separate model for each state\n",
    "old_states = feature_df['state']\n",
    "\n",
    "# Make dummies for states\n",
    "feature_df = pd.get_dummies(feature_df, columns=['state'])\n",
    "feature_df['state'] = old_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make our models that we want to train\n",
    "# Parameters for XGBClassifier\n",
    "xgb_params = {\n",
    "  'max_depth': [3, 5, 7, 9], \n",
    "  'n_estimators': [30, 50, 100, 300]\n",
    "}\n",
    "\n",
    "# Parameters for LogisitcRegression\n",
    "logi_regr_params = {\n",
    "    'penalty': ['l1', 'l2'],\n",
    "    'C': [1e-2, 1e-1, 1, 10, 1e3, 1e5]\n",
    "}\n",
    "\n",
    "# Parameters for RandomForest\n",
    "random_forest_params = {\n",
    "  'max_depth': [3, 5, 7, 9],\n",
    "  'n_estimators': [30, 50, 100, 300]\n",
    "}\n",
    "\n",
    "# Parameters for AdaBoost\n",
    "adaboost_params = {\n",
    "  'n_estimators': [30, 50, 100, 300]\n",
    "}\n",
    "\n",
    "knn_params = {\n",
    "    'n_neighbors': [3, 5, 7],\n",
    "    'algorithm': ['ball_tree', 'kd_tree']\n",
    "}\n",
    "\n",
    "# Parameters for GaussianNB\n",
    "percent_positive = feature_df['label'].mean() # Percentage of positive labels\n",
    "percent_negative = 1 - percent_positive # Percentage of negative features \n",
    "bayes_params = {'priors': [None, [percent_negative, percent_positive]]}\n",
    "\n",
    "\n",
    "# Create a dictionary of models with names as keys\n",
    "# model{ 'model name': (model_object, parameters) } \n",
    "models = {\n",
    "    'XGBoost': (XGBClassifier(), xgb_params), \n",
    "    'Logistic Reg': (LogisticRegression(), logi_regr_params),\n",
    "    'Random Forest': (RandomForestClassifier(), random_forest_params),\n",
    "    'AdaBoost': (AdaBoostClassifier(), adaboost_params),\n",
    "    'KNN' : (KNeighborsClassifier(), knn_params),\n",
    "    'Gaussian NB': (GaussianNB(), bayes_params)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_year = 2016\n",
    "(training_history, testing_history, \n",
    "testing_history_probs, trained_models) = train_models(feature_df, models, test_year, extra_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>state</th>\n",
       "      <th>this_year</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Alabama</td>\n",
       "      <td>1999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Alabama</td>\n",
       "      <td>2000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Alabama</td>\n",
       "      <td>2001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Alabama</td>\n",
       "      <td>2002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Alabama</td>\n",
       "      <td>2003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Alabama</td>\n",
       "      <td>2004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Alabama</td>\n",
       "      <td>2005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Alabama</td>\n",
       "      <td>2006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Alabama</td>\n",
       "      <td>2007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Alabama</td>\n",
       "      <td>2008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Alabama</td>\n",
       "      <td>2009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Alabama</td>\n",
       "      <td>2010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Alabama</td>\n",
       "      <td>2011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Alabama</td>\n",
       "      <td>2012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Alabama</td>\n",
       "      <td>2013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Alabama</td>\n",
       "      <td>2014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Alabama</td>\n",
       "      <td>2015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Alabama</td>\n",
       "      <td>2016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Alaska</td>\n",
       "      <td>1999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Alaska</td>\n",
       "      <td>2000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>Alaska</td>\n",
       "      <td>2001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>Alaska</td>\n",
       "      <td>2002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>Alaska</td>\n",
       "      <td>2003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>Alaska</td>\n",
       "      <td>2004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>Alaska</td>\n",
       "      <td>2005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>Alaska</td>\n",
       "      <td>2006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>Alaska</td>\n",
       "      <td>2007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>Alaska</td>\n",
       "      <td>2008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>Alaska</td>\n",
       "      <td>2009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>Alaska</td>\n",
       "      <td>2010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>870</th>\n",
       "      <td>Wisconsin</td>\n",
       "      <td>2005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>871</th>\n",
       "      <td>Wisconsin</td>\n",
       "      <td>2006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>872</th>\n",
       "      <td>Wisconsin</td>\n",
       "      <td>2007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>873</th>\n",
       "      <td>Wisconsin</td>\n",
       "      <td>2008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>874</th>\n",
       "      <td>Wisconsin</td>\n",
       "      <td>2009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>875</th>\n",
       "      <td>Wisconsin</td>\n",
       "      <td>2010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>876</th>\n",
       "      <td>Wisconsin</td>\n",
       "      <td>2011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>877</th>\n",
       "      <td>Wisconsin</td>\n",
       "      <td>2012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>878</th>\n",
       "      <td>Wisconsin</td>\n",
       "      <td>2013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>879</th>\n",
       "      <td>Wisconsin</td>\n",
       "      <td>2014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>880</th>\n",
       "      <td>Wisconsin</td>\n",
       "      <td>2015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>881</th>\n",
       "      <td>Wisconsin</td>\n",
       "      <td>2016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>882</th>\n",
       "      <td>Wyoming</td>\n",
       "      <td>1999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>883</th>\n",
       "      <td>Wyoming</td>\n",
       "      <td>2000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>884</th>\n",
       "      <td>Wyoming</td>\n",
       "      <td>2001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>885</th>\n",
       "      <td>Wyoming</td>\n",
       "      <td>2002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>886</th>\n",
       "      <td>Wyoming</td>\n",
       "      <td>2003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>887</th>\n",
       "      <td>Wyoming</td>\n",
       "      <td>2004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>888</th>\n",
       "      <td>Wyoming</td>\n",
       "      <td>2005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>889</th>\n",
       "      <td>Wyoming</td>\n",
       "      <td>2006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>890</th>\n",
       "      <td>Wyoming</td>\n",
       "      <td>2007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>891</th>\n",
       "      <td>Wyoming</td>\n",
       "      <td>2008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>892</th>\n",
       "      <td>Wyoming</td>\n",
       "      <td>2009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>893</th>\n",
       "      <td>Wyoming</td>\n",
       "      <td>2010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>894</th>\n",
       "      <td>Wyoming</td>\n",
       "      <td>2011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>895</th>\n",
       "      <td>Wyoming</td>\n",
       "      <td>2012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>896</th>\n",
       "      <td>Wyoming</td>\n",
       "      <td>2013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>897</th>\n",
       "      <td>Wyoming</td>\n",
       "      <td>2014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>898</th>\n",
       "      <td>Wyoming</td>\n",
       "      <td>2015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>899</th>\n",
       "      <td>Wyoming</td>\n",
       "      <td>2016</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>900 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         state  this_year\n",
       "0      Alabama       1999\n",
       "1      Alabama       2000\n",
       "2      Alabama       2001\n",
       "3      Alabama       2002\n",
       "4      Alabama       2003\n",
       "5      Alabama       2004\n",
       "6      Alabama       2005\n",
       "7      Alabama       2006\n",
       "8      Alabama       2007\n",
       "9      Alabama       2008\n",
       "10     Alabama       2009\n",
       "11     Alabama       2010\n",
       "12     Alabama       2011\n",
       "13     Alabama       2012\n",
       "14     Alabama       2013\n",
       "15     Alabama       2014\n",
       "16     Alabama       2015\n",
       "17     Alabama       2016\n",
       "18      Alaska       1999\n",
       "19      Alaska       2000\n",
       "20      Alaska       2001\n",
       "21      Alaska       2002\n",
       "22      Alaska       2003\n",
       "23      Alaska       2004\n",
       "24      Alaska       2005\n",
       "25      Alaska       2006\n",
       "26      Alaska       2007\n",
       "27      Alaska       2008\n",
       "28      Alaska       2009\n",
       "29      Alaska       2010\n",
       "..         ...        ...\n",
       "870  Wisconsin       2005\n",
       "871  Wisconsin       2006\n",
       "872  Wisconsin       2007\n",
       "873  Wisconsin       2008\n",
       "874  Wisconsin       2009\n",
       "875  Wisconsin       2010\n",
       "876  Wisconsin       2011\n",
       "877  Wisconsin       2012\n",
       "878  Wisconsin       2013\n",
       "879  Wisconsin       2014\n",
       "880  Wisconsin       2015\n",
       "881  Wisconsin       2016\n",
       "882    Wyoming       1999\n",
       "883    Wyoming       2000\n",
       "884    Wyoming       2001\n",
       "885    Wyoming       2002\n",
       "886    Wyoming       2003\n",
       "887    Wyoming       2004\n",
       "888    Wyoming       2005\n",
       "889    Wyoming       2006\n",
       "890    Wyoming       2007\n",
       "891    Wyoming       2008\n",
       "892    Wyoming       2009\n",
       "893    Wyoming       2010\n",
       "894    Wyoming       2011\n",
       "895    Wyoming       2012\n",
       "896    Wyoming       2013\n",
       "897    Wyoming       2014\n",
       "898    Wyoming       2015\n",
       "899    Wyoming       2016\n",
       "\n",
       "[900 rows x 2 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "annual_df[['state', 'this_year']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "all_preds = []\n",
    "for v in testing_history.values():\n",
    "    all_preds.append(v)\n",
    "    \n",
    "all_preds = np.array(all_preds).T\n",
    "vote_by_preds = [int(x > 0.5) for x in all_preds.mean(axis=1)]\n",
    "\n",
    "testing_history['vote_by_preds'] = vote_by_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBoost: 0.64 \n",
      "Recall: 0.2857142857142857**\n",
      "Precision: 0.2222222222222222\n",
      "[[58 21]\n",
      " [15  6]]\n",
      "----------\n",
      "Logistic Reg: 0.6 \n",
      "Recall: 0.23809523809523808**\n",
      "Precision: 0.1724137931034483\n",
      "[[55 24]\n",
      " [16  5]]\n",
      "----------\n",
      "Random Forest: 0.64 \n",
      "Recall: 0.23809523809523808**\n",
      "Precision: 0.2\n",
      "[[59 20]\n",
      " [16  5]]\n",
      "----------\n",
      "AdaBoost: 0.65 \n",
      "Recall: 0.3333333333333333**\n",
      "Precision: 0.25\n",
      "[[58 21]\n",
      " [14  7]]\n",
      "----------\n",
      "KNN: 0.71 \n",
      "Recall: 0.047619047619047616**\n",
      "Precision: 0.1\n",
      "[[70  9]\n",
      " [20  1]]\n",
      "----------\n",
      "Gaussian NB: 0.45 \n",
      "Recall: 0.6666666666666666**\n",
      "Precision: 0.22580645161290322\n",
      "[[31 48]\n",
      " [ 7 14]]\n",
      "----------\n",
      "meta: 0.62 \n",
      "Recall: 0.3333333333333333**\n",
      "Precision: 0.22580645161290322\n",
      "[[55 24]\n",
      " [14  7]]\n",
      "----------\n",
      "vote_by_preds: 0.66 \n",
      "Recall: 0.2857142857142857**\n",
      "Precision: 0.24\n",
      "[[60 19]\n",
      " [15  6]]\n",
      "----------\n"
     ]
    }
   ],
   "source": [
    "date_filter = (feature_df['next_year'] >= 2016)\n",
    "truth = feature_df.loc[date_filter, 'label']\n",
    "\n",
    "testing_results_df = pd.DataFrame(feature_df.loc[date_filter, ['label', 'next_date']])\n",
    "for name, preds in testing_history.items():\n",
    "    # Add predictions to feature_df\n",
    "    testing_results_df[name] = preds == truth\n",
    "    \n",
    "    # Get accuracy score and confusion matrix\n",
    "    print(\"{}: {} \".format(name, accuracy_score(truth, preds)))\n",
    "    cm = confusion_matrix(truth, preds)\n",
    "    tn, fp, fn, tp = cm[0][0], cm[0][1], cm[1][0], cm[1][1]\n",
    "    recall = tp / (tp + fn)\n",
    "    precision = tp / (tp + fp)\n",
    "#     print(\"True Negative: {}\".format(tn))\n",
    "#     print(\"False Positive: {}\".format(fp))\n",
    "#     print(\"False Negative: {}\".format(fn))\n",
    "#     print(\"True Positive: {}\".format(tp))\n",
    "    print(\"Recall: {}**\".format(recall))\n",
    "    print(\"Precision: {}\".format(precision))\n",
    "    print(cm)\n",
    "    print('-'*10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 4: Annual Modeling \n",
    "In this notebook, we will try modeling gun violence trends annually. Since we have more annual data than monthly data, our goal is to see whether the annual features will be more helpful here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import defaultdict\n",
    "from collections import defaultdict\n",
    "\n",
    "# Numpy and pandas for manipulating the data\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Matplotlib and seaborn for visualization\n",
    "import matplotlib as mpl\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# GridSearchCV for training \n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Performance metrics from sklearn\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# Prophet for time forecasting\n",
    "from fbprophet import Prophet\n",
    "\n",
    "# Classification models\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "# To hide stdout because Prophet can be loud\n",
    "import logging\n",
    "logging.getLogger('fbprophet').setLevel(logging.WARNING)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "annual_file = './data/cleaned/annual.csv'\n",
    "by_date_total_file = './data/cleaned/by_date_total.csv'\n",
    "provisions_file = './data/raw/provisions.csv'\n",
    "useful_singles_file = './data/cleaned/useful_singles.csv'\n",
    "useful_doubles_file = './data/cleaned/useful_doubles.csv'\n",
    "\n",
    "annual_df = pd.read_csv(annual_file, parse_dates=True, index_col=0)\n",
    "by_date_total_df = pd.read_csv(by_date_total_file, parse_dates=True, index_col=0)\n",
    "provisions_df = pd.read_csv(provisions_file, parse_dates=True)\n",
    "useful_singles_df = pd.read_csv(useful_singles_file, parse_dates=True, index_col=0)\n",
    "useful_doubles_df = pd.read_csv(useful_doubles_file, parse_dates=True, index_col=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fixing up features\n",
    "As before, there are still a couple of features to be tweaked. We need to remove redundant columns and add the label and the provisions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Drop District of Columbia\n",
    "annual_df = annual_df[annual_df['state'] != 'District of Columbia']\n",
    "\n",
    "# Drop redundant columns\n",
    "annual_df = annual_df.drop(['gun_deaths_norm', 'other_crime_norm'], axis=1)\n",
    "\n",
    "# Get states that we will be making models for\n",
    "states = annual_df['state'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create columns for next year's gun deaths (what we want to predict for each row)\n",
    "next_annual_df = pd.DataFrame()\n",
    "next_annual_df['state'] = annual_df['state']\n",
    "next_annual_df['this_year'] = annual_df['year'] - 1\n",
    "next_annual_df['next_year'] = annual_df['year']\n",
    "next_annual_df['next_gun_deaths'] = annual_df['gun_deaths']\n",
    "\n",
    "# Merge so we have next year's gun deaths in each row\n",
    "annual_df = pd.merge(annual_df, next_annual_df,\n",
    "         left_on=['year', 'state'], right_on=['this_year', 'state'])\n",
    "\n",
    "# Drop year, as it is now named this_year\n",
    "annual_df = annual_df.drop('year', axis=1)\n",
    "\n",
    "# Add features from last year in order to view short time trend\n",
    "features_to_add = annual_df.drop(['this_year', 'next_year',\n",
    "                                  'state', 'next_gun_deaths'], axis=1).columns\n",
    "\n",
    "last_annual_df = pd.DataFrame()\n",
    "last_annual_df['state'] = annual_df['state']\n",
    "last_annual_df['this_year'] = annual_df['this_year'] + 1\n",
    "last_annual_df['last_year'] = annual_df['this_year']\n",
    "for feature in features_to_add:\n",
    "    last_annual_df['last_' + feature] = annual_df[feature]\n",
    "\n",
    "annual_df = pd.merge(annual_df, last_annual_df,\n",
    "                    left_on=['this_year', 'state'], right_on=['this_year', 'state'])\n",
    "\n",
    "# Replace last year value with change from last year\n",
    "for feature in features_to_add:\n",
    "    current = annual_df[feature]\n",
    "    last = annual_df['last_' + feature]\n",
    "    annual_df[feature + '_change'] = (current - last) / np.clip(last, 1, None)\n",
    "    annual_df = annual_df.drop('last_' + feature, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "157"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a label (gun deaths increases by more than 10%)\n",
    "rate_change = (annual_df['next_gun_deaths'] - annual_df['gun_deaths'] ) / annual_df['gun_deaths']\n",
    "annual_df['label'] = rate_change > 0.2\n",
    "annual_df['label'].sum() # See how many positives we have"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Add only the useful provisions to our feature_df (k from this year and k from n years prior)\n",
    "def add_provisions(annual_df, provisions_df, useful_singles_df, \n",
    "                   useful_doubles_df, k_s=10, k_d=10, n=5):\n",
    "    \n",
    "    # Get the state and year columns for a join later and lawtotal to account for excluded provisions\n",
    "    singles = list(useful_singles_df.head(k_s)['provision'].values)\n",
    "    doubles = list(useful_doubles_df.head(k_d)['provision'].values)\n",
    "    others = ['year', 'state', 'lawtotal'] # Add lawtotal and the year, state for merging \n",
    "    columns = singles + doubles + others\n",
    "    \n",
    "    # Extend provisions_df to include pairs\n",
    "    provisions = provisions_df.columns[2:-1]\n",
    "    for i in range(provisions.shape[0]):\n",
    "        for j in range(i, provisions.shape[0]):\n",
    "            # p1 p2 the two provisions to use\n",
    "            p1 = provisions[i]\n",
    "            p2 = provisions[j]\n",
    "            provisions_df[p1 + '_' + p2] = provisions_df[p1] * provisions_df[p2]\n",
    "  \n",
    "    # Get the years \n",
    "    years = annual_df.groupby('this_year').count().index.values\n",
    "\n",
    "    # Keep track of provisions for this year and n years prior\n",
    "    current_provisions = []\n",
    "    old_provisions = []\n",
    "\n",
    "    # Add the provisions from each year to a list\n",
    "    for year in years:\n",
    "        current_filter_df = provisions_df[provisions_df['year'] == year]\n",
    "        old_filter_df = provisions_df[provisions_df['year'] == year - n]\n",
    "        \n",
    "        # Add the columns\n",
    "        current_provisions.append(current_filter_df[columns])\n",
    "        old_provisions.append(old_filter_df[columns])\n",
    "        \n",
    "    # Put the provisions into a DataFrame\n",
    "    current_provisions = pd.concat(current_provisions)\n",
    "    old_provisions = pd.concat(old_provisions)\n",
    "    old_provisions['year'] += n # Match the year which we want to join onto\n",
    "\n",
    "    # Merge the provisions\n",
    "    all_provisions = pd.merge(current_provisions, old_provisions, \n",
    "                              on=['state', 'year'], suffixes=('', '_old'))\n",
    "\n",
    "    # Add provisions to feature_df and return the new feature_df\n",
    "    annual_df = pd.merge(annual_df, all_provisions, \n",
    "                          left_on=['this_year', 'state'], \n",
    "                          right_on=['year', 'state'])\n",
    "    return annual_df.drop('year', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Add provisions to our annual_df\n",
    "annual_df = add_provisions(annual_df, provisions_df, useful_singles_df, \n",
    "                            useful_doubles_df, k_s=10, k_d=20, n=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Make a function to train our models and return the results\n",
    "def train_models(annual_df, models, test_year, extra_columns):\n",
    "    \"\"\" Function to train models, returning test and train predictions and trained models.\n",
    "    feature_df   (DataFrame): Pandas DataFrame with all of the features, including the label\n",
    "    \n",
    "    models            (dict): dict with model names as keys and model, params pairs as values \n",
    "    \n",
    "    test_year          (int): Year to test on\n",
    "    \n",
    "    extra_columns     (list): List of columns to drop before training (columns that would either \n",
    "    not help with the predictions, or would be cheating by using the label itself). \n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    # Initialize the dictionaries we will be returning later\n",
    "    training_history = defaultdict(list)\n",
    "    testing_history = defaultdict(list)\n",
    "    testing_history_probs = defaultdict(list)\n",
    "    trained_models = defaultdict(dict)\n",
    "    \n",
    "    # Train XGBoost for next_year 2000 to 2013; make predictions for 2014-2017\n",
    "    model, parameters = models['XGBoost']\n",
    "    \n",
    "    pretrain_preds = []\n",
    "    for year in [2014, 2015, 2016, 2017]:\n",
    "        train_filter = annual_df['next_year'] < year\n",
    "        test_filter = annual_df['next_year'] == year\n",
    "    \n",
    "        X_train = annual_df.dropna(axis=1).loc[train_filter].drop(extra_columns, axis=1).values\n",
    "        y_train = annual_df.loc[train_filter, 'label']\n",
    "\n",
    "        X_test = annual_df.dropna(axis=1).loc[test_filter].drop(extra_columns, axis=1).values\n",
    "        y_test = annual_df.loc[train_filter, 'label']\n",
    "            \n",
    "        clf = GridSearchCV(model, parameters)\n",
    "        clf.fit(X_train, y_train)\n",
    "\n",
    "        # Make predictions on training set\n",
    "        test_preds = clf.predict(X_test)\n",
    "        pretrain_preds.extend(test_preds)\n",
    "\n",
    "    \n",
    "    feature_df = annual_df.dropna()\n",
    "    print(feature_df.shape)\n",
    "    print(len(pretrain_preds))\n",
    "    feature_df['pretrain'] = pretrain_preds\n",
    "    \n",
    "    # For each state, train a new set of models\n",
    "    # Training data is all data before next_year\n",
    "    # Testing data all data during next_year\n",
    "    train_filter = feature_df['next_year'] < test_year\n",
    "    test_filter = feature_df['next_year'] >= test_year\n",
    "\n",
    "    # Partition the feature_df for the training and testing sets\n",
    "    X_train = feature_df.loc[train_filter].drop(extra_columns, axis=1).values\n",
    "    y_train = feature_df.loc[train_filter, 'label']\n",
    "\n",
    "    # Note that the test set has only ONE row for each state. \n",
    "    X_test = feature_df.loc[test_filter].drop(extra_columns, axis=1).values\n",
    "    y_test = feature_df.loc[test_filter, 'label']\n",
    "\n",
    "    # Keep track of predictions so we can train the meta model as well\n",
    "    meta_train = []\n",
    "    meta_test = []\n",
    "    for name, (model, parameters) in models.items():\n",
    "        clf = GridSearchCV(model, parameters)\n",
    "        clf.fit(X_train, y_train)\n",
    "\n",
    "        # Make predictions on training set\n",
    "        train_preds = clf.predict(X_train)\n",
    "        test_preds = clf.predict(X_test)\n",
    "        train_probs = clf.best_estimator_.predict_proba(X_train)[:, 0]\n",
    "        test_probs = clf.best_estimator_.predict_proba(X_test)[:, 0]\n",
    "\n",
    "        # Make meta features to train the meta model on\n",
    "        meta_train.append(train_probs)\n",
    "        meta_test.append(test_probs)\n",
    "\n",
    "        # Keep track of the predictions\n",
    "        training_history[name].append(train_preds)\n",
    "        testing_history[name].extend(test_preds)\n",
    "        testing_history_probs[name].extend(test_probs)\n",
    "\n",
    "        # Remember the last model\n",
    "        trained_models[name] = clf\n",
    "\n",
    "    # Take transpose of meta features so that observations are rows\n",
    "    meta_train = np.array(meta_train).T\n",
    "    meta_test = np.array(meta_test).T\n",
    "\n",
    "    # Create and train the meta model\n",
    "    clf = GridSearchCV(XGBClassifier(), xgb_params)\n",
    "    clf.fit(meta_train, y_train)\n",
    "\n",
    "    # Make training and testing predictions\n",
    "    train_preds = clf.predict(meta_train)\n",
    "    test_preds = clf.predict(meta_test)\n",
    "\n",
    "    # Keep track of the predictions\n",
    "    training_history['meta'].append(train_preds)\n",
    "    testing_history['meta'].extend(test_preds)\n",
    "    testing_history_probs['meta'].extend(test_probs)\n",
    "    return training_history, testing_history, testing_history_probs, trained_models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Training models\n",
    "At this point we should make an important decision. We have to decide the years which we will be training on, as some of our data is not available for earlier years. In this first test, I decided to drop the features that we have insufficient data for, and just training on the features that have complete data for all years."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Prepare feature_df and extra columns\n",
    "extra_columns = ['label', 'next_gun_deaths', 'state']\n",
    "\n",
    "# Make dummies for states\n",
    "# annual_df = pd.get_dummies(annual_df, columns=['state'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Make our models that we want to train\n",
    "# Parameters for XGBClassifier\n",
    "xgb_params = {\n",
    "  'max_depth': [3, 5, 7, 9], \n",
    "  'n_estimators': [30, 50, 100, 300]\n",
    "}\n",
    "\n",
    "# Parameters for LogisitcRegression\n",
    "logi_regr_params = {\n",
    "    'penalty': ['l1', 'l2'],\n",
    "    'C': [1e-2, 1e-1, 1, 10, 1e3, 1e5]\n",
    "}\n",
    "\n",
    "# Parameters for RandomForest\n",
    "random_forest_params = {\n",
    "  'max_depth': [3, 5, 7, 9],\n",
    "  'n_estimators': [30, 50, 100, 300]\n",
    "}\n",
    "\n",
    "# Parameters for AdaBoost\n",
    "adaboost_params = {\n",
    "  'n_estimators': [30, 50, 100, 300]\n",
    "}\n",
    "\n",
    "knn_params = {\n",
    "    'n_neighbors': [3, 5, 7],\n",
    "    'algorithm': ['ball_tree', 'kd_tree']\n",
    "}\n",
    "\n",
    "# Parameters for GaussianNB\n",
    "percent_positive = annual_df['label'].mean() # Percentage of positive labels\n",
    "percent_negative = 1 - percent_positive # Percentage of negative features \n",
    "bayes_params = {'priors': [None, [percent_negative, percent_positive]]}\n",
    "\n",
    "\n",
    "# Create a dictionary of models with names as keys\n",
    "# model{ 'model name': (model_object, parameters) } \n",
    "models = {\n",
    "    'XGBoost': (XGBClassifier(), xgb_params), \n",
    "    'Logistic Reg': (LogisticRegression(), logi_regr_params),\n",
    "    'Random Forest': (RandomForestClassifier(), random_forest_params),\n",
    "    'AdaBoost': (AdaBoostClassifier(), adaboost_params),\n",
    "    'KNN' : (KNeighborsClassifier(), knn_params),\n",
    "    'Gaussian NB': (GaussianNB(), bayes_params)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(200, 138)\n",
      "200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jshuai/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:46: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    }
   ],
   "source": [
    "test_year = 2016\n",
    "(training_history, testing_history, \n",
    "testing_history_probs, trained_models) = train_models(annual_df, models, test_year, extra_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "all_preds = []\n",
    "for v in testing_history.values():\n",
    "    all_preds.append(v)\n",
    "    \n",
    "all_preds = np.array(all_preds).T\n",
    "vote_by_preds = [int(x > 0.5) for x in all_preds.mean(axis=1)]\n",
    "\n",
    "testing_history['vote_by_preds'] = vote_by_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Lengths must match to compare",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-cd117de40e36>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpreds\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mtruth\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/pandas/core/ops.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(self, other, axis)\u001b[0m\n\u001b[1;32m    826\u001b[0m             if (not is_scalar(lib.item_from_zerodim(other)) and\n\u001b[1;32m    827\u001b[0m                     len(self) != len(other)):\n\u001b[0;32m--> 828\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Lengths must match to compare'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    829\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    830\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mother\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mABCPeriodIndex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Lengths must match to compare"
     ]
    }
   ],
   "source": [
    "np.array(preds) == truth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "invalid type comparison",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-5b228471b399>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreds\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtesting_history\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;31m# Add predictions to feature_df\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0mtesting_results_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpreds\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mtruth\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;31m# Get accuracy score and confusion matrix\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/pandas/core/ops.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(self, other, axis)\u001b[0m\n\u001b[1;32m    859\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    860\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merrstate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'ignore'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 861\u001b[0;31m                 \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mna_op\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mother\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    862\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mis_scalar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mres\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    863\u001b[0m                 raise TypeError('Could not compare %s type with Series' %\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/pandas/core/ops.py\u001b[0m in \u001b[0;36mna_op\u001b[0;34m(x, y)\u001b[0m\n\u001b[1;32m    798\u001b[0m                     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    799\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mresult\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNotImplemented\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 800\u001b[0;31m                     \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"invalid type comparison\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    801\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    802\u001b[0m                 \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: invalid type comparison"
     ]
    }
   ],
   "source": [
    "date_filter = (annual_df['next_year'] >= 2017)\n",
    "truth = annual_df.loc[date_filter, 'label']\n",
    "\n",
    "testing_results_df = pd.DataFrame(annual_df.loc[date_filter, ['label', 'next_date']])\n",
    "for name, preds in testing_history.items():\n",
    "    \n",
    "    # Add predictions to feature_df\n",
    "    testing_results_df[name] = preds == truth\n",
    "    \n",
    "    # Get accuracy score and confusion matrix\n",
    "    print(\"{}: {} \".format(name, accuracy_score(truth, preds)))\n",
    "    cm = confusion_matrix(truth, preds)\n",
    "    tn, fp, fn, tp = cm[0][0], cm[0][1], cm[1][0], cm[1][1]\n",
    "    recall = tp / (tp + fn)\n",
    "    precision = tp / (tp + fp)\n",
    "#     print(\"True Negative: {}\".format(tn))\n",
    "#     print(\"False Positive: {}\".format(fp))\n",
    "#     print(\"False Negative: {}\".format(fn))\n",
    "#     print(\"True Positive: {}\".format(tp))\n",
    "    print(\"Recall: {}**\".format(recall))\n",
    "    print(\"Precision: {}\".format(precision))\n",
    "    print(cm)\n",
    "    print('-'*10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importances_df = pd.DataFrame()\n",
    "importances_df['feature'] = annual_df.drop(extra_columns, axis=1).columns\n",
    "importances = trained_models['XGBoost'].best_estimator_.feature_importances_\n",
    "importances_df['importance'] = importances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importances"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
